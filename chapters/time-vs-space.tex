\chapter{Leveraging Time vs Space Division Multiplexing}\label{sec:leveraging-time-division-multiplexing}

When a chunk of data is available at a given time for processing, it can all be processed at the same time, or pieces after pieces.
Processing simultaneously all the data offers the advantage of a reduced latency, but requires creating copies of the processing 
blocks for each piece. A sequential processing gives the possibility of having just one processing block treating the data step 
by step at a price of latency.

A simple parallel with human body is our hands. When writing a text, because of our small number of hands we are condemned to write 
texts one letter after another. But if we imagined having 1000 hands, and enough space to hold as many pen without interfering with one another,
we could imagine writing whole paragraphs at once.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{time_vs_space_mux.png}
    \caption{Simple Representation of Time and Space Division Multiplexing}
    \label{fig:time-vs-space}
\end{figure}

\section{Time Division Multiplexing}\label{sec:time-division-multiplexing}
The time division multiplexing approach consists in sharing the resources, or processing blocks, by routing each piece of data 
depending on time \(t\). In the figure \ref{fig:time-vs-space} this refers to the lower block diagram. Choosing this implementation 
can be intuitive as computers usually process data using frames of 32 to 64 bits. It has the advantage of reusing design blocks hence
possibly reducing area usage, but performs poorly in case of data dependencies. If the word from time \(t=0\) of the frame needs to be 
available at the same time as the one of \(t=4\), the logic becomes very complex thus canceling the positive area impact unlocked by 
resource sharing.

This sequential processing approach requires counters used by multiplexers to latch the input data at the right moments.

\section{Space Division Multiplexing}
Another way of treating the data is to make it all available at the same time, as parallel inputs of the processing block. This approach
is more natural as it is closer to what humans experience; we see, hear, feel, taste at the same time, not one after the other. In figure \ref{fig:time-vs-space}
it is represented on top. Space Division Multiplexing generally offers small latencies, as every block runs in parallel and reduces the use of 
sequential logic which would be required in TDM. In fact, using a parallel logic, some processing blocks can become purely combinational ones.

\section{Tradeoffs}\label{sec:space-versus-time-division-multiplexing}
Whether one approach is better than the other is impossible to determine prior to having a deep understanding of the data processing blocks.
The most common tradeoff is the one of latency at the price of area. But the Maximal Operating Frequency (\(F_{max}\)) can also be traded to 
improve other metrics.

Having access to all variables at the same time makes the code development easier.


\section{Leveraging Both Approaches for LATOME}
In the version 6 firmware the choice was made to use parallel data as much as possible as it showed promising preliminary results. The input and output interfaces of both ISM and OSUM are indeed fully parallel.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{osum_v6.png}
    \caption{Output Summing Block Diagram}
    \label{fig:osum-v6}
\end{figure}

The Output Summing represented in figure \ref{fig:osum-v6} is made of the following blocks:
\begin{itemize}
    \item Masking: responsible for disabling some inputs depending on the detector region that a specific LATOME treats
    \item Emec Adapter: this block performs additional summing for the LATOME responsible for the EMEC region of the detector
    \item Multi Linear Encoder (MLE): Since the ADC readings are 12 bits long but the FEX systems requires 10 bits long energy levels, this block encodes the ADC readings on 10 bits
    \item Data Encoder: Once the encoded energy levels are ready, this block creates 17 frames of 224 bits representing the whole data
    \item Output Switch Matrix (OSM): This switch matrix routes the 17 224 bits frames to the different FEX systems that the LATOME is connected too
    \item Frame Alignment: Each couple of BC, the LATOME does not send any data but instead it sends an alignment frame built by this block
    \item CRC9: This final block appends to the last 9 bits of each frame a CRC9 value which it computes
\end{itemize}

When investigating the OSM block more in details, it appears that, in the worst case, there should be 48 multiplexers of input size \(224\times17=3808\).
By looking at the data path and optimizing for it, the biggest multiplexers have at most 4 input frames.
Figure \ref{fig:osum-v6} shows the parallel to serial block creating 32 bits words for the FEX systems. Moving this block to the left would have no impact
on the latency. However, by moving this block before the Output Switch Matrix, the biggest multiplexers would have an input size of \(4\times32\) instead of \(4\times224\).

An important aspect to monitor is the data dependencies, since these could have a critical impact if using TDM. The OSM block is a routing block which does not change any data in the frames. The Frame Alignment only overwrites the data that it gets as input if the current BC index is an alignment one, hence it does not show data dependencies. Finally the Cyclic Redundant Check can be both computed in a parallel or serial fashion equivalently without any effect on performances.
This first example shows a practical case where TDM could have a beneficial effect on the firmware design in terms of optimization, without having to trade off any other metric.

